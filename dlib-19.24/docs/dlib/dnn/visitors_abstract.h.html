<html><!-- Created using the cpp_pretty_printer from the dlib C++ library.  See http://dlib.net for updates. --><head><title>dlib C++ Library - visitors_abstract.h</title></head><body bgcolor='white'><pre>
<font color='#009900'>// Copyright (C) 2022  Davis E. King (davis@dlib.net)
</font><font color='#009900'>// License: Boost Software License   See LICENSE.txt for the full license.
</font><font color='#0000FF'>#undef</font> DLIB_DNn_VISITORS_ABSTRACT_H_
<font color='#0000FF'>#ifdef</font> DLIB_DNn_VISITORS_ABSTRACT_H_

<font color='#0000FF'>#include</font> "<a style='text-decoration:none' href='input.h.html'>input.h</a>"
<font color='#0000FF'>#include</font> "<a style='text-decoration:none' href='layers.h.html'>layers.h</a>"
<font color='#0000FF'>#include</font> "<a style='text-decoration:none' href='loss.h.html'>loss.h</a>"

<font color='#0000FF'>namespace</font> dlib
<b>{</b>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='set_all_bn_running_stats_window_sizes'></a>set_all_bn_running_stats_window_sizes</b> <font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        <font color='#0000FF'><u>unsigned</u></font> <font color='#0000FF'><u>long</u></font> new_window_size
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - new_window_size &gt; 0
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
        ensures
            - Sets the get_running_stats_window_size() field of all bn_ layers in net to
              new_window_size.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='fuse_layers'></a>fuse_layers</b> <font face='Lucida Console'>(</font>
        net_type<font color='#5555FF'>&amp;</font> net
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
            - net has been properly allocated, that is: count_parameters(net) &gt; 0.
        ensures
            - Disables all the affine_ layers that have a convolution as an input.
            - Updates the convolution weights beneath the affine_ layers to produce the same
              output as with the affine_ layers enabled.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='net_to_xml'></a>net_to_xml</b> <font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        std::ostream<font color='#5555FF'>&amp;</font> out
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
            - All layers in the net must provide to_xml() functions.
        ensures
            - Prints the given neural network object as an XML document to the given output
              stream.
    !*/</font>

    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='net_to_xml'></a>net_to_xml</b> <font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        <font color='#0000FF'>const</font> std::string<font color='#5555FF'>&amp;</font> filename
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
            - All layers in the net must provide to_xml() functions.
        ensures
            - This function is just like the above net_to_xml(), except it writes to a file
              rather than an ostream.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    dpoint <b><a name='input_tensor_to_output_tensor'></a>input_tensor_to_output_tensor</b><font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        dpoint p 
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_skip_layer, or add_tag_layer.
            - All layers in the net must provide map_input_to_output() functions.
        ensures
            - Given a dpoint (i.e. a row,column coordinate) in the input tensor given to
              net, this function returns the corresponding dpoint in the output tensor
              net.get_output().  This kind of mapping is useful when working with fully
              convolutional networks as you will often want to know what parts of the
              output feature maps correspond to what parts of the input.
            - If the network contains skip layers then any layers skipped over by the skip
              layer are ignored for the purpose of computing this coordinate mapping.  That
              is, if you walk the network from the output layer to the input layer, where
              each time you encounter a skip layer you jump to the layer indicated by the
              skip layer, you will visit exactly the layers in the network involved in the
              input_tensor_to_output_tensor() calculation. This behavior is useful since it
              allows you to compute some auxiliary DNN as a separate branch of computation
              that is separate from the main network's job of running some kind of fully
              convolutional network over an image.  For instance, you might want to have a
              branch in your network that computes some global image level
              summarization/feature.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    dpoint <b><a name='output_tensor_to_input_tensor'></a>output_tensor_to_input_tensor</b><font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        dpoint p  
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_skip_layer, or add_tag_layer.
            - All layers in the net must provide map_output_to_input() functions.
        ensures
            - This function provides the reverse mapping of input_tensor_to_output_tensor().
              That is, given a dpoint in net.get_output(), what is the corresponding dpoint
              in the input tensor?
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'>inline</font> <font color='#0000FF'><u>size_t</u></font> <b><a name='count_parameters'></a>count_parameters</b><font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
        ensures
            - Returns the number of allocated parameters in the network. E.g. if the network has not
              been trained then, since nothing has been allocated yet, it will return 0.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font><font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='set_all_learning_rate_multipliers'></a>set_all_learning_rate_multipliers</b><font face='Lucida Console'>(</font>
        net_type<font color='#5555FF'>&amp;</font> net,
        <font color='#0000FF'><u>double</u></font> learning_rate_multiplier
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
            - learning_rate_multiplier &gt;= 0
        ensures
            - Sets all learning_rate_multipliers and bias_learning_rate_multipliers in net
              to learning_rate_multiplier.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'><u>size_t</u></font> begin, <font color='#0000FF'><u>size_t</u></font> end, <font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='set_learning_rate_multipliers_range'></a>set_learning_rate_multipliers_range</b><font face='Lucida Console'>(</font>
        net_type<font color='#5555FF'>&amp;</font> net,
        <font color='#0000FF'><u>double</u></font> learning_rate_multiplier
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
            - learning_rate_multiplier &gt;= 0
            - begin &lt;= end &lt;= net_type::num_layers
        ensures
            - Loops over the layers in the range [begin,end) in net and calls
              set_learning_rate_multiplier on them with the value of
              learning_rate_multiplier.
    !*/</font>

<font color='#009900'>// ----------------------------------------------------------------------------------------
</font>
    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='net_to_dot'></a>net_to_dot</b> <font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        std::ostream<font color='#5555FF'>&amp;</font> out
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
        ensures
            - Prints the given neural network object as an DOT document to the given output
              stream.
            - The contents of #out can be used by the dot program from Graphviz to export
              the network diagram to any supported format.
    !*/</font>

    <font color='#0000FF'>template</font> <font color='#5555FF'>&lt;</font><font color='#0000FF'>typename</font> net_type<font color='#5555FF'>&gt;</font>
    <font color='#0000FF'><u>void</u></font> <b><a name='net_to_dot'></a>net_to_dot</b> <font face='Lucida Console'>(</font>
        <font color='#0000FF'>const</font> net_type<font color='#5555FF'>&amp;</font> net,
        <font color='#0000FF'>const</font> std::string<font color='#5555FF'>&amp;</font> filename
    <font face='Lucida Console'>)</font>;
    <font color='#009900'>/*!
        requires
            - net_type is an object of type add_layer, add_loss_layer, add_skip_layer, or
              add_tag_layer.
        ensures
            - This function is just like the above net_to_dot(), except it writes to a file
              rather than an ostream.
    !*/</font>
<b>}</b>

<font color='#0000FF'>#endif</font> <font color='#009900'>// DLIB_DNn_VISITORS_ABSTRACT_H_
</font>
</pre></body></html>